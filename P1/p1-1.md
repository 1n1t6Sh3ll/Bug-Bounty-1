## Using Default Credentials

### How to Test

1. The first step is to determine what software is being used.
2. Once the software has been identified, try to find whether it uses default passwords, and if so, what they are. This should include:

    * Searching for “[SOFTWARE] default password”.
    * Reviewing the manual or vendor documentation.
    * Checking common default password databases, such as [CIRT.net](https://cirt.net/passwords), [SecLists Default Passwords](https://github.com/danielmiessler/SecLists/tree/master/Passwords/Default-Credentials) or [DefaultCreds-cheat-sheet.](https://owasp.org/www-project-web-security-testing-guide/latest/4-Web_Application_Security_Testing/04-Authentication_Testing/02-Testing_for_Default_Credentials)
    * Inspecting the application source code (if available).
    * Installing the application on a virtual machine and inspecting it.
    * Inspecting the physical hardware for stickers (often present on network devices).   

#### Refrence
[Testing for Default Credentials](https://owasp.org/www-project-web-security-testing-guide/latest/4-Web_Application_Security_Testing/04-Authentication_Testing/02-Testing_for_Default_Credentials)

### Writeups 
[How I got $13337 bounty From Google](https://thesecurityexperts.wordpress.com/2017/09/24/how-i-got-13337-bounty-from-google/)

### H1 Reports

[Default Admin Username and Password](https://hackerone.com/reports/1195325)

[Unauthorized access to employee panel with default credentials](https://hackerone.com/reports/1063298)

[DVR default username and password](https://hackerone.com/reports/398797)


### Bugcrowd Reports

[Grafana admin login via default credentials](https://bugcrowd.com/disclosures/f810da90-2aff-4970-b6b9-09a471e1b805/grafana-admin-login-via-default-credentials)

[Unauthorised Admin Access Due to default Password](https://bugcrowd.com/disclosures/82bb3923-4097-4a64-a5f7-d5f6e59f1b6d/unauthorised-admin-access-due-to-default-password)

### Videos
[Using Default Credentials](https://www.youtube.com/watch?v=EeYq2r-ZI-Q)

[Admin access default credentials](https://www.youtube.com/watch?v=HBECQNJ9ok0)

[Grafana admin login via default credentials](https://www.youtube.com/watch?v=vAAwhQAmsgI)

import tensorflow as tf 
import numpy as np 
from tensorflow.keras.preprocessing.text import Tokenizer 
from tensorflow.keras.preprocessing.sequence import pad_sequences 
emails = [ "Buy cheap watches! Free shipping!", 
"Meeting for lunch today?", 
"Claim your prize! You've won $1,000,000!", 
"Important meeting at 3 PM.", 
] 
Labels = [1, 0, 1, 0] 
print(emails) 
max_words = 1000 
max_len = 50 
tokenizer = Tokenizer(num_words=max_words, oov_token="<00V>") 
tokenizer.fit_on_texts(emails) 
sequences = tokenizer.texts_to_sequences(emails) 
x_padded = pad_sequences(sequences, maxlen=max_len, padding='post', truncating="post") 
print(x_padded) 
print(sequences) 
model=tf.keras.Sequential([ 
tf.keras.layers.Embedding(input_dim=max_words, output_dim=16, input_length=max_len), 
tf.keras.layers.Flatten(), 
tf.keras.layers.Dense(6, activation='relu'), 
tf.keras.layers.Dense(1, activation='sigmoid') 
]) 
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) 
training_data = np.array(x_padded) 
training_labels = np.array(Labels) 
model.fit(training_data, training_labels, epochs=50) 
file_path=r'D:/MSC.IT/SEM 3/Web Data Analytics/Practicals/WB Sem3/NSpam.txt' 
with open(file_path, "r", encoding="utf-8") as file: 
 sample_email_text =file.read()
sequences_sample=tokenizer.texts_to_sequences ([sample_email_text]) 
sample_email_padded=pad_sequences (sequences_sample, maxlen=max_len, padding="post", truncating="post") 
prediction= model.predict(sample_email_padded) 
threshold = 0.5 
if prediction > threshold: 
 print(f"Sample Email ('{file_path}'): The Email is SPAM")
else: 
 print(f"Sample Email ('{file_path}): NOT SPAM") 

aprioir algo
from mlxtend.frequent_patterns import apriori 
from mlxtend.frequent_patterns import association_rules 
import pandas as pd
 dataset= [ 
 ['milk','bread','nuts'], 
 ['milk','bread'], 
 ['milk','eggs','nuts'], 
 ['milk','bread','eggs'], 
 ['bread','nuts'], 
 ] 
 df=pd.DataFrame(dataset) 
 print("\n Transaction database:") 
 print(df) 
 df_encoded=pd.get_dummies(df,prefix='',prefix_sep='') 
 print("\n Transaction encoded:") 
 print(df_encoded) 
 frequent_itemsets=apriori(df_encoded,min_support=0.5,use_colnames=True) 
 print("\n Frequent itemsets:") 
 print(frequent_itemsets) 
 rules=association_rules(freqent_itemsets,metric='confidence',min_threshold=0.5) 
 print("\n Association rules:") 
 print(rules)

crawler
 import collections 
 collections.Callable = collections.abc.Callable
 import requests 
 from bs4 import BeautifulSoup 
 imprt re 
 def crawl_and_search(url,keyword):
 try: 
 response = requests.get(url) 
 response.raise_for_status() 
 page_content = response.text 
 soup = BeautifulSoup(page_content,'html.parser')
 text = soup.get_text() 
 if re.search(keyword,text,re.IGNORECASE):
 print(f"keyowrd'{keyword}'found in {url}")
 else: 
 print(f"keyowrd'{keyword}'not found in {url}")
 except requests.exceptions.RequestException as e:
 print(f"Failed to retrieve {url}:{e}")
 url=input("Enter the url to crawl") 
 keyword=input("Enter the keyword")
 crawl_and_search(url,keyword) 

 pagerank algo
 import networkx as nx 
 G=nx.random_k_out_graph(n=8,k=2,alpha=0.75)
 def draw_grapd(G): 
 nx.draw(G, with_labels=True, font_weight='bold',node_size=400)
 draw_grapd(G) 
 ranks_pr=nx.pagerank(G) 
 print("PageRank:") 
 print(ranks_pr)

scrap data from web. 
 import collections 
 collections.Callable = collections.abc.Callable
 import requests 
 from bs4 import BeautifulSoup 
 def check_word_in_webpage(url,word):
 response=requests.get(url) 
 if response.status_code==200: 
 soup=BeautifulSoup(response.content,'html.parser')
 text_content=soup.get_text() 
 if word.lower() in text_content.lower():
 print(f"The word '{word}' is present in the webpage")
 else: 
 print(f"The word '{word}' is not present in the webpage")
 else: 
 print("Failed to retrieve webpage")
 url=input("Enter the url") 
 word_to_check=input("Enter the word")
 check_word_in_webpage(url,word_to_check)

focused crawler
import praw 
import pandas as pd 
#after adding above mentioned than only it will work
reddit = praw.Reddit(client_id='i4d9pgm0q0my7JmJjbOWAw', 
client_secret='qvp3KTqxty8abmJsYLzRBmd4M7sYDw', user_agent='Loud
sub_name = input("enter the Keyword: ") 
max_posts = 5 
reddit.read_only = True 
title=[] 
for submission in reddit.subreddit(sub_name).new(limit=max_posts): 
 title.append(submission.title) 
for t in range(len(title)): 
 print(title[t]) 

 
